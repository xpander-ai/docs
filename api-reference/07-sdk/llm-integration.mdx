---
title: "LLM Integration"
description: "Integrating different LLM providers with the xpander.ai SDK"
icon: "microchip"
---

The xpander.ai SDK is designed to work with multiple Large Language Model (LLM) providers. This guide explains how to integrate various LLM providers with your xpander.ai agents.

## Supported LLM Providers

The SDK supports the following LLM providers through the `LLMProvider` enum:

| Provider         | Enum Value                     | Format                            |
|------------------|--------------------------------|-----------------------------------|
| OpenAI           | `LLMProvider.OPEN_AI`          | Standard OpenAI format            |
| FriendliAI       | `LLMProvider.FRIENDLI_AI`      | FriendliAI format (Claude, etc.)  |
| Gemini           | `LLMProvider.GEMINI_OPEN_AI`   | Google Gemini with OpenAI format  |
| Ollama           | `LLMProvider.OLLAMA`           | Ollama format for local models    |
| LangChain        | `LLMProvider.LANG_CHAIN`       | LangChain format                  |
| Real-time OpenAI | `LLMProvider.REAL_TIME_OPEN_AI` | Real-time OpenAI format          |
| NVIDIA NIM       | `LLMProvider.NVIDIA_NIM`       | NVIDIA NIM format                 |
| Amazon Bedrock   | `LLMProvider.AMAZON_BEDROCK`   | Amazon Bedrock format             |

## Integration Basics

Integrating an LLM provider involves three key steps:

1. **Initialize memory** - Initialize the agent's memory with execution input and instructions
2. **Format tools** - Get tools formatted for the specific LLM provider
3. **Extract tool calls** - Parse the LLM response to extract tool calls

### Provider Specification Requirements

> **Important**: The xpander.ai SDK requires explicit provider specification when getting tools and extracting tool calls. Always specify the `llm_provider` parameter with the appropriate provider enum value.

<Steps>
  <Step title="Initialize memory">
    ```python
    from xpander_sdk import LLMProvider
    
    # Initialize memory - provider specification is optional
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )
    ```
  </Step>
  <Step title="Get tools formatted for your LLM provider">
    ```python
    # Get tools in the format required by your specific LLM provider
    # Always specify the provider to get the correct format
    openai_tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
    claude_tools = agent.get_tools(llm_provider=LLMProvider.FRIENDLI_AI)
    gemini_tools = agent.get_tools(llm_provider=LLMProvider.GEMINI_OPEN_AI)
    ```
  </Step>
  <Step title="Extract tool calls with explicit provider">
    ```python
    # Always explicitly specify the provider when extracting tool calls
    # This must match the provider used for getting tools
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI
    )
    ```
  </Step>
</Steps>

## Provider-Specific Integration

<Tabs>
  <Tab title="OpenAI">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    from openai import OpenAI
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

    # Initialize clients
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )

    # Run until completion
    while not agent.is_finished():
        # Get OpenAI-formatted tools
        tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
        
        # Call OpenAI
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=agent.messages,
            tools=tools,
            tool_choice="auto",
            temperature=0.0
        )
        
        # Add response to memory
        agent.add_messages(messages=response.model_dump())
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.OPEN_AI
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
  </Tab>
  <Tab title="FriendliAI (Claude)">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    import anthropic
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    FRIENDLI_API_KEY = os.environ["FRIENDLI_API_KEY"]

    # Initialize clients
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)
    friendli_client = anthropic.Anthropic(api_key=FRIENDLI_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )

    # Run until completion
    while not agent.is_finished():
        # Get FriendliAI-formatted tools
        tools = agent.get_tools(llm_provider=LLMProvider.FRIENDLI_AI)
        
        # Call FriendliAI (using Claude model here)
        response = friendli_client.messages.create(
            model="claude-3-opus-20240229",
            messages=agent.messages,
            tools=tools,
            temperature=0.0
        )
        
        # Add response to memory
        agent.add_messages(messages=response.model_dump())
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.FRIENDLI_AI
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
  </Tab>
  <Tab title="Google Gemini">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    from openai import OpenAI
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    GEMINI_API_KEY = os.environ["GEMINI_API_KEY"]

    # Initialize clients
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)

    # Initialize Gemini using OpenAI compatibility API
    gemini_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
    )

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )

    # Run until completion
    while not agent.is_finished():
        # Get Gemini-formatted tools
        tools = agent.get_tools(llm_provider=LLMProvider.GEMINI_OPEN_AI)
        
        # Call Gemini via OpenAI compatibility API
        response = gemini_client.chat.completions.create(
            model="gemini-1.5-pro",
            messages=agent.messages,
            tools=tools,
            temperature=0.0
        )
        
        # Add response to memory
        agent.add_messages(messages=response.model_dump())
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response.model_dump(),
            llm_provider=LLMProvider.GEMINI_OPEN_AI
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
  </Tab>
  <Tab title="Ollama">
    ```python
    from xpander_sdk import XpanderClient, LLMProvider
    import requests
    import json
    from dotenv import load_dotenv
    import os

    # Load environment variables
    load_dotenv()
    XPANDER_API_KEY = os.environ["XPANDER_API_KEY"]
    OLLAMA_BASE_URL = "http://localhost:11434"  # Default Ollama URL

    # Initialize client
    xpander_client = XpanderClient(api_key=XPANDER_API_KEY)

    # Get agent
    agent = xpander_client.agents.get(agent_id="agent-1234")

    # Add task and initialize memory
    agent.add_task(input="What are the latest developments in AI?")
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )

    # Helper function to call Ollama API
    def call_ollama(messages, tools=None, model="llama3:latest"):
        url = f"{OLLAMA_BASE_URL}/api/chat"
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": 0.0
            }
        }
        
        # Add tools if available
        if tools:
            payload["tools"] = tools
        
        response = requests.post(url, json=payload)
        return response.json()

    # Run until completion
    while not agent.is_finished():
        # Get Ollama-formatted tools - must specify the provider here
        tools = agent.get_tools(llm_provider=LLMProvider.OLLAMA)
        
        # Call Ollama
        ollama_response = call_ollama(
            messages=agent.messages,
            tools=tools,
            model="llama3:latest"  # Use your preferred model
        )
        
        # Format response to standard structure
        response_dict = {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": ollama_response.get("message", {}).get("content", ""),
                    "tool_calls": ollama_response.get("message", {}).get("tool_calls", [])
                }
            }]
        }
        
        # Add response to memory
        agent.add_messages(messages=response_dict)
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response_dict,
            llm_provider=LLMProvider.OLLAMA
        )
        
        # Execute tools if any
        if tool_calls:
            results = agent.run_tools(tool_calls=tool_calls)
            print(f"Executed {len(results)} tools")

    # Get final result
    result = agent.retrieve_execution_result()
    print(result.result)
    ```
  </Tab>
</Tabs>

## Best Practices

<Tabs>
  <Tab title="Provider Specification">
    Always specify the correct provider for all operations:

    ```python
    # For retrieving tools
    tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)

    # For extracting tool calls
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI
    )
    ```

    Use the same provider for both retrieving tools and extracting tool calls:

    ```python
    # Use the same provider in both places
    tools = agent.get_tools(llm_provider=LLMProvider.OPEN_AI)
    # ...call LLM...
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI  # Same provider as used for tools
    )
    ```
  </Tab>
  <Tab title="Memory Management">
    Memory initialization doesn't require provider specification:

    ```python
    # Provider specification is optional for memory initialization
    agent.memory.init_messages(
        input=agent.execution.input_message,
        instructions=agent.instructions
    )
    ```

    When working with multiple providers, always track which provider is being used:

    ```python
    # Define providers
    openai_provider = LLMProvider.OPEN_AI
    claude_provider = LLMProvider.FRIENDLI_AI

    # Function that includes provider tracking
    def process_with_provider(agent, response, provider):
        # Add messages to memory
        agent.add_messages(messages=response)
        
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response,
            llm_provider=provider
        )
        
        # Run tools
        return agent.run_tools(tool_calls=tool_calls)
    ```
  </Tab>
  <Tab title="Error Handling">
    Implement proper error handling for LLM API calls and tool call extraction:

    ```python
    try:
        # Extract tool calls with explicit provider
        tool_calls = XpanderClient.extract_tool_calls(
            llm_response=response,
            llm_provider=LLMProvider.OPEN_AI
        )
        
        # Handle case where no tool calls were found
        if not tool_calls and expected_tools:
            print("No tool calls found in response")
            # Handle the situation appropriately
            
    except Exception as e:
        print(f"Error extracting tool calls: {str(e)}")
        # Handle the error or use a default response
    ```

    Implement rate limiting to avoid hitting API limits:

    ```python
    import time

    # Add a small delay between API calls
    time.sleep(1)
    ```
  </Tab>
</Tabs>

## Message Format Handling

The xpander.ai SDK automatically handles all message format conversions for all LLM providers:

<Tabs>
  <Tab title="AI State Machine">
    xpander.ai SDK stores all messages in a standard format in `agent.messages`. This format is compatible with all the LLM Providers.

    ```json
    [
      {
        "role": "system",
        "content": "System instructions..."
      },
      {
        "role": "user",
        "content": "User query..."
      },
      {
        "role": "assistant",
        "content": "Assistant response...",
        "tool_calls": [...]  // If any tools were called
      }
    ]
    ```
  </Tab>
  <Tab title="Format Conversion">
    When using any LLM provider, the SDK handles conversions automatically:

    ```python
    # For any provider (OpenAI, Claude, Gemini, etc.)
    # No manual conversion needed - just specify the right LLMProvider when getting tools

    # For OpenAI
    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=agent.messages,  # Use directly, no conversion needed
        tools=agent.get_tools(llm_provider=LLMProvider.OPEN_AI),
        temperature=0
    )

    # For Claude
    response = claude_client.messages.create(
        model="claude-3-opus-20240229",
        messages=agent.messages,  # Use directly, no conversion needed
        tools=agent.get_tools(llm_provider=LLMProvider.FRIENDLI_AI),
        temperature=0
    )
    ```

    For adding responses back to memory:

    ```python
    # For any provider, add response directly
    # The SDK automatically handles format detection and normalization
    agent.add_messages(messages=response)

    # For OpenAI client library which provides model_dump()
    agent.add_messages(messages=response.model_dump())
    ```
  </Tab>
  <Tab title="Tool Call Extraction">
    When extracting tool calls, just specify the right provider:

    ```python
    # Extract tool calls by specifying the provider
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response,
        llm_provider=LLMProvider.FRIENDLI_AI  # Specify the provider
    )
    ```

    > **Important:** You don't need to manually convert messages between formats. The SDK automatically handles all conversions internally.
  </Tab>
</Tabs>

## Related Resources

<CardGroup cols={2}>
  <Card title="Agent" icon="robot" href="/api-reference/07-sdk/agent">
    Learn about the core Agent class
  </Card>
  <Card title="Memory Management" icon="brain" href="/api-reference/07-sdk/agent/memory">
    Managing agent memory
  </Card>
  <Card title="Agent Tools" icon="wrench" href="/api-reference/07-sdk/agent/tools">
    Working with agent tools
  </Card>
  <Card title="Tool Models" icon="box" href="/api-reference/07-sdk/models/tool-models">
    Understanding tool data models
  </Card>
</CardGroup> 