---
title: "Memory Management"
description: "Managing conversation history and context for AI agents"
icon: "brain"
---

The xpander.ai SDK provides a Memory system for agents to manage conversation history, including user inputs, system instructions, and tool call results. Memory is a critical component that allows agents to maintain context during task execution.

## Accessing Memory

Memory is accessed through the `memory` property of an Agent instance.

<CodeGroup>
```python Python
from xpander_sdk import XpanderClient

# Initialize client
client = XpanderClient(api_key="your-api-key")

# Get an agent
agent = client.agents.get(agent_id="agent-id")

# Access memory
memory = agent.memory
```

```typescript TypeScript
import { XpanderClient } from '@xpander-ai/sdk';

// Initialize client
const client = new XpanderClient({ apiKey: "your-api-key" });

// Get an agent (async function)
async function accessMemory() {
  const agent = await client.agents.get({ agentId: "agent-id" });
  
  // Access memory
  const memory = agent.memory;
}
```
</CodeGroup>

## Key Methods

### init_messages() / initMessages()

Initializes the agent's memory with system instructions and a user input message. This method must be called before attempting to access or modify the agent's messages.

<CodeGroup>
```python Python
from xpander_sdk import LLMProvider

# Initialize memory after adding a task
agent.memory.init_messages(
    input=agent.execution.input_message,
    instructions=agent.instructions,
    llm_provider=LLMProvider.OPEN_AI
)

# Check that messages are now available
print(f"Number of messages: {len(agent.messages)}")
```

```typescript TypeScript
import { LLMProvider } from '@xpander-ai/sdk';

// Initialize memory after adding a task
await agent.memory.initMessages({
    input: agent.execution.inputMessage,
    instructions: agent.instructions,
    llmProvider: LLMProvider.OPEN_AI
});

// Check that messages are now available
console.log(`Number of messages: ${agent.messages.length}`);
```
</CodeGroup>

#### Parameters

| Parameter      | Type         | Required | Description                                        |
|----------------|--------------|----------|----------------------------------------------------|
| `input`        | string       | Yes      | User input message to initialize with              |
| `instructions` | string       | Yes      | System instructions for the agent                  |
| `llm_provider` | LLMProvider  | No       | LLM provider format (default: LLMProvider.OPEN_AI) |

<Note>
The `init_messages()` method sets up the initial conversation state with system instructions and user input. It must be called before any other memory operations.
</Note>

### add_messages() / addMessages()

Adds messages (typically LLM responses) to the agent's memory.

<CodeGroup>
```python Python
# Get response from OpenAI
response = openai_client.chat.completions.create(
    model="gpt-4o",
    messages=agent.messages,
    tools=agent.get_tools(),
    temperature=0.0
)

# Add LLM response to memory
agent.memory.add_messages(messages=response.model_dump())

print(f"Added response from LLM to memory")
```

```typescript TypeScript
// Get response from OpenAI
const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: agent.messages,
    tools: agent.getTools({ llmProvider: LLMProvider.OPEN_AI }),
    temperature: 0
});

// Add LLM response to memory
await agent.memory.addMessages({ messages: response });

console.log("Added response from LLM to memory");
```
</CodeGroup>

#### Parameters

| Parameter  | Type | Required | Description                               |
|------------|------|----------|-------------------------------------------|
| `messages` | Dict | Yes      | Messages object to add (e.g., LLM response)|

### add_tool_call_results() / addToolCallResults()

Adds tool call results to the agent's memory. This updates the conversation state with the results of executed tools.

<CodeGroup>
```python Python
from xpander_sdk import ToolCallResult

# Create tool call results
tool_results = [
    ToolCallResult(
        function_name="get_weather",
        tool_call_id="call-123",
        is_success=True,
        result="The weather in New York is sunny with a temperature of 72°F.",
        payload={"location": "New York"}
    )
]

# Add tool results to memory
agent.memory.add_tool_call_results(tool_call_results=tool_results)

# Check updated message count
print(f"Message count after tool results: {len(agent.messages)}")
```

```typescript TypeScript
import { ToolCallResult } from '@xpander-ai/sdk';

// Create tool call results
const toolResults = [
    new ToolCallResult({
        functionName: "get_weather",
        toolCallId: "call-123",
        isSuccess: true,
        result: "The weather in New York is sunny with a temperature of 72°F.",
        payload: { location: "New York" }
    })
];

// Add tool results to memory
await agent.memory.addToolCallResults({ toolCallResults: toolResults });

// Check updated message count
console.log(`Message count after tool results: ${agent.messages.length}`);
```
</CodeGroup>

#### Parameters

| Parameter          | Type                    | Required | Description                 |
|--------------------|--------------------------|---------|-----------------------------|
| `tool_call_results`| List[ToolCallResult]    | Yes     | Results of tool executions  |

## Working with Messages

After initializing memory, the agent's `messages` property contains the conversation history in a format compatible with LLM providers.

<CodeGroup>
```python Python
# Initialize memory first
agent.memory.init_messages(
    input="Tell me about AI agents",
    instructions=agent.instructions,
    llm_provider=LLMProvider.OPEN_AI
)

# Access messages
for message in agent.messages:
    print(f"Role: {message['role']}")
    print(f"Content: {message.get('content', '')[:50]}...")
    print("---")

# Use messages directly with OpenAI
response = openai_client.chat.completions.create(
    model="gpt-4o",
    messages=agent.messages,
    temperature=0.7
)
```

```typescript TypeScript
// Initialize memory first
await agent.memory.initMessages({
    input: "Tell me about AI agents",
    instructions: agent.instructions,
    llmProvider: LLMProvider.OPEN_AI
});

// Access messages
for (const message of agent.messages) {
    console.log(`Role: ${message.role}`);
    console.log(`Content: ${message.content?.substring(0, 50)}...`);
    console.log("---");
}

// Use messages directly with OpenAI
const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: agent.messages,
    temperature: 0.7
});
```
</CodeGroup>

## Message Format

The format of messages depends on the LLM provider specified during initialization. For OpenAI (the default), messages follow this structure:

<Expandable title="OpenAI Message Format">
```json
[
  {
    "role": "system",
    "content": "You are a helpful assistant..."
  },
  {
    "role": "user",
    "content": "Tell me about AI agents"
  },
  {
    "role": "assistant",
    "content": "AI agents are software entities that can...",
    "tool_calls": [...]  // Optional
  },
  {
    "role": "tool", // Added when tools are executed
    "tool_call_id": "call_1234",
    "content": "The weather in New York is sunny with a temperature of 72°F."
  }
]
```
</Expandable>

## Complete Example

Here's a complete example of memory management during an agent execution cycle:

<CodeGroup>
```python Python
from xpander_sdk import XpanderClient, LLMProvider
from openai import OpenAI
import os

# Initialize clients
xpander_client = XpanderClient(api_key=os.environ["XPANDER_API_KEY"])
openai_client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# Get agent
agent = xpander_client.agents.get(agent_id=os.environ["AGENT_ID"])

# Add a task
agent.add_task(input="What's the weather like in New York?")

# Initialize memory with OpenAI format
agent.memory.init_messages(
    input=agent.execution.input_message,
    instructions=agent.instructions,
    llm_provider=LLMProvider.OPEN_AI
)

print(f"Initial message count: {len(agent.messages)}")

# Run the agent loop
while not agent.is_finished():
    # Get next action from LLM
    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=agent.messages,
        tools=agent.get_tools(),
        tool_choice="auto",
        temperature=0.0
    )
    
    # Add LLM response to memory
    agent.memory.add_messages(messages=response.model_dump())
    print(f"Message count after LLM response: {len(agent.messages)}")
    
    # Extract tool calls
    tool_calls = XpanderClient.extract_tool_calls(
        llm_response=response.model_dump(),
        llm_provider=LLMProvider.OPEN_AI
    )
    
    # Execute tools if any
    if tool_calls:
        results = agent.run_tools(tool_calls=tool_calls)
        print(f"Executed {len(results)} tools")
        print(f"Message count after tool execution: {len(agent.messages)}")

# Final message count
print(f"Final message count: {len(agent.messages)}")
print("Final message types:")

for i, message in enumerate(agent.messages):
    print(f"{i}: Role: {message.get('role')}, Has content: {'content' in message}")
```

```typescript TypeScript
import { XpanderClient, LLMProvider } from '@xpander-ai/sdk';
import OpenAI from 'openai';
import * as dotenv from 'dotenv';

// Load environment variables
dotenv.config();

async function runMemoryExample() {
    // Initialize clients
    const xpanderClient = new XpanderClient({ apiKey: process.env.XPANDER_API_KEY });
    const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    
    // Get agent
    const agent = await xpanderClient.agents.get({ agentId: process.env.AGENT_ID });
    
    // Add a task
    await agent.addTask({ input: "What's the weather like in New York?" });
    
    // Initialize memory with OpenAI format
    await agent.memory.initMessages({
        input: agent.execution.inputMessage,
        instructions: agent.instructions,
        llmProvider: LLMProvider.OPEN_AI
    });
    
    console.log(`Initial message count: ${agent.messages.length}`);
    
    // Run the agent loop
    while (!(await agent.isFinished())) {
        // Get next action from LLM
        const response = await openai.chat.completions.create({
            model: "gpt-4o",
            messages: agent.messages,
            tools: agent.getTools({ llmProvider: LLMProvider.OPEN_AI }),
            tool_choice: "auto",
            temperature: 0
        });
        
        // Add LLM response to memory
        await agent.memory.addMessages({ messages: response });
        console.log(`Message count after LLM response: ${agent.messages.length}`);
        
        // Extract tool calls
        const toolCalls = XpanderClient.extractToolCalls({
            llmResponse: response,
            llmProvider: LLMProvider.OPEN_AI
        });
        
        // Execute tools if any
        if (toolCalls.length > 0) {
            const results = await agent.runTools({ toolCalls });
            console.log(`Executed ${results.length} tools`);
            console.log(`Message count after tool execution: ${agent.messages.length}`);
        }
    }
    
    // Final message count
    console.log(`Final message count: ${agent.messages.length}`);
    console.log("Final message types:");
    
    agent.messages.forEach((message, i) => {
        console.log(`${i}: Role: ${message.role}, Has content: ${message.content !== undefined}`);
    });
}

runMemoryExample().catch(console.error);
```
</CodeGroup>

## Memory Management Tips

### Message Limits

LLM providers have token limits for input context. When working with long conversations:

1. **Monitor token usage** - Count the approximate number of tokens in your messages
2. **Implement summarization** - Consider summarizing previous conversation history for very long conversations
3. **Use thread_id parameter** - When adding tasks, use the same thread_id to continue conversations

### Best Practices

<CodeGroup>
```python Python
# Good practice: always initialize memory after adding a task
agent.add_task(input="What are the best programming languages for AI?")
agent.memory.init_messages(
    input=agent.execution.input_message,
    instructions=agent.instructions,
    llm_provider=LLMProvider.OPEN_AI
)

# Good practice: ensure messages exist before accessing them
if hasattr(agent, 'messages') and len(agent.messages) > 0:
    print(f"First message role: {agent.messages[0]['role']}")
```

```typescript TypeScript
// Good practice: always initialize memory after adding a task
await agent.addTask({ input: "What are the best programming languages for AI?" });
await agent.memory.initMessages({
    input: agent.execution.inputMessage,
    instructions: agent.instructions,
    llmProvider: LLMProvider.OPEN_AI
});

// Good practice: ensure messages exist before accessing them
if (agent.messages && agent.messages.length > 0) {
    console.log(`First message role: ${agent.messages[0].role}`);
}
```
</CodeGroup>

## Related Resources

<CardGroup cols={2}>
  <Card title="Agent Class" icon="robot" href="/api-reference/07-sdk/agent">
    Learn about the core Agent class
  </Card>
  <Card title="Agent Tools" icon="wrench" href="/api-reference/07-sdk/agent/tools">
    Managing tool usage with agents
  </Card>
  <Card title="LLM Integration" icon="microchip" href="/api-reference/07-sdk/llm-integration">
    Working with different LLM providers
  </Card>
  <Card title="Execution Management" icon="play" href="/api-reference/07-sdk/agent/execution">
    Managing agent executions and tasks
  </Card>
</CardGroup> 